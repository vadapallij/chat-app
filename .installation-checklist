CHAT-APP INSTALLATION CHECKLIST
================================

Follow this checklist to ensure everything is set up correctly.

PREREQUISITES:
â–¡ NVIDIA RTX 5090 GPU installed and recognized
â–¡ NVIDIA drivers installed (check: nvidia-smi)
â–¡ CUDA 12.x installed (check: nvcc --version)
â–¡ Go 1.23+ installed (check: go version)
â–¡ Python 3.8+ installed (check: python3 --version)
â–¡ pipx installed (check: pipx --version)

POSTGRESQL SETUP:
â–¡ PostgreSQL installed (check: psql --version)
â–¡ PostgreSQL service running (check: sudo systemctl status postgresql)
â–¡ Can connect to PostgreSQL (check: psql -l)
â–¡ Database 'chat_app' created (run: createdb chat_app)
â–¡ Migrations applied (run: psql chat_app < migrations/001_init.sql)
â–¡ Tables verified (run: psql chat_app -c "\dt")

VLLM SETUP:
â–¡ Old vLLM uninstalled (run: pipx uninstall vllm)
â–¡ Fresh vLLM installed (run: pipx install vllm)
â–¡ vLLM command available (check: vllm --version)
â–¡ HuggingFace CLI available (check: pipx run huggingface-cli --version)
â–¡ HuggingFace account created
â–¡ HuggingFace token obtained (from: https://huggingface.co/settings/tokens)
â–¡ Logged into HuggingFace (run: pipx run huggingface-cli login)
â–¡ Access to Llama 3.1 granted (visit: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)

ENVIRONMENT CONFIGURATION:
â–¡ .env.example exists in project directory
â–¡ .env file created (run: cp .env.example .env)
â–¡ DATABASE_URL configured in .env
â–¡ VLLM_BASE_URL configured in .env (default: http://localhost:5000)
â–¡ PORT configured in .env (default: 8080)

GO APPLICATION:
â–¡ Go dependencies downloaded (run: go mod download)
â–¡ Application compiles (run: go build)
â–¡ services/vllm.go exists
â–¡ main.go references vLLM service

RUNTIME VERIFICATION:
â–¡ vLLM server starts successfully (run: ./start-vllm-rtx5090.sh)
â–¡ vLLM downloads Llama 3.1 8B model (~16GB, first run only)
â–¡ vLLM shows "Uvicorn running on http://0.0.0.0:5000"
â–¡ vLLM responds to health check (run: curl http://localhost:5000/v1/models)
â–¡ Chat app starts successfully (run: go run main.go)
â–¡ Chat app shows "Starting server on port 8080"
â–¡ Chat app responds to health check (run: curl http://localhost:8080/health)
â–¡ Browser can access http://localhost:8080
â–¡ Can create new conversation in UI
â–¡ Can send message and receive AI response
â–¡ GPU shows activity during inference (run: nvidia-smi)

OPTIONAL OPTIMIZATIONS:
â–¡ Prefix caching enabled in vLLM
â–¡ GPU memory utilization set to 0.95
â–¡ Max model length set to 32768
â–¡ bfloat16 precision enabled

TROUBLESHOOTING RESOURCES:
â–¡ Read SETUP_SUMMARY.md
â–¡ Read QUICK_START.md
â–¡ Read VLLM_SETUP.md for vLLM issues
â–¡ Read POSTGRESQL_SETUP.md for database issues

COMPLETION:
â–¡ Everything works end-to-end
â–¡ Can have a conversation with the AI
â–¡ Ready to customize and extend!

======================================
If all boxes are checked, you're done!
ðŸš€ Enjoy your local AI chat app!
======================================
